{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Web Scraping EMSERH - Vers√£o Final Corrigida\n",
    "#\n",
    "# 1. Executar todas as c√©lulas em ordem\n",
    "# 2. Configurar par√¢metros no bloco CONFIG\n",
    "\n",
    "# %% [code]\n",
    "%pip install requests beautifulsoup4 pandas tqdm --quiet\n",
    "\n",
    "# %% [code]\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.colab import files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes otimizadas\n",
    "CONFIG = {\n",
    "    'base_url': 'https://www.emserh.ma.gov.br',\n",
    "    'max_pages': 576,          # Altere para 579 quando validar\n",
    "    'request_delay': 0.5,     # Delay entre requisi√ß√µes (segundos)\n",
    "    'timeout': 30,\n",
    "    'max_workers': 3,         # Threads paralelas\n",
    "    'backup_interval': 50,    # Backup a cada X artigos\n",
    "    'json_params': {\n",
    "        'indent': 2,\n",
    "        'ensure_ascii': False  # Mant√©m caracteres especiais\n",
    "    }\n",
    "}\n",
    "\n",
    "# %% [code]\n",
    "def get_news_links():\n",
    "    \"\"\"Coleta links com tratamento robusto de erros\"\"\"\n",
    "    links = []\n",
    "    print(f'üîç Coletando links de {CONFIG[\"max_pages\"]} p√°ginas...')\n",
    "\n",
    "    try:\n",
    "        for page in tqdm(range(1, CONFIG['max_pages'] + 1), desc='P√°ginas'):\n",
    "            try:\n",
    "                url = f\"{CONFIG['base_url']}/noticias/page/{page}/\"\n",
    "                response = requests.get(url, timeout=CONFIG['timeout'])\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                card_body = soup.find('div', class_='card-body')\n",
    "\n",
    "                if card_body:\n",
    "                    for article in card_body.find_all('div', class_='row mt-3'):\n",
    "                        if (a := article.find('a', href=True)):\n",
    "                            link = a['href']\n",
    "                            if not link.startswith('http'):\n",
    "                                link = f\"{CONFIG['base_url']}{link}\" if link.startswith('/') else f\"{CONFIG['base_url']}/{link}\"\n",
    "                            links.append(link)\n",
    "\n",
    "                time.sleep(CONFIG['request_delay'])\n",
    "\n",
    "            except Exception as e:\n",
    "                tqdm.write(f'üö® Erro na p√°gina {page}: {str(e)}')\n",
    "                continue\n",
    "\n",
    "        return list(dict.fromkeys(links))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Falha cr√≠tica na coleta de links: {str(e)}')\n",
    "        return []\n",
    "\n",
    "# %% [code]\n",
    "def scrape_article(link):\n",
    "    \"\"\"Vers√£o corrigida com serializa√ß√£o HTML adequada\"\"\"\n",
    "    try:\n",
    "        response = requests.get(link, timeout=CONFIG['timeout'])\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extra√ß√£o segura de elementos\n",
    "        autor = data_publicacao = titulo = conteudo = None\n",
    "\n",
    "        # Autor\n",
    "        if (svg := soup.find('svg', class_='fa-user')):\n",
    "            autor = svg.find_next_sibling('span').get_text(strip=True) if svg.find_next_sibling('span') else None\n",
    "\n",
    "        # T√≠tulo (serializa√ß√£o correta)\n",
    "        if (card_header := soup.find('div', class_='card-header')):\n",
    "            h1 = card_header.find('h1')\n",
    "            if h1:\n",
    "                h1_clone = BeautifulSoup(str(h1), 'html.parser')\n",
    "                for img in h1_clone.find_all('img'):\n",
    "                    img.decompose()\n",
    "                titulo = h1_clone.decode_contents()  # Corrige escape de tags\n",
    "\n",
    "        # Conte√∫do (serializa√ß√£o correta)\n",
    "        conteudo = []\n",
    "        if (card_body := soup.find('div', class_='card-body')):\n",
    "            for p in card_body.find_all('p'):\n",
    "                if not p.find_parent('div', class_='wp-block-image'):\n",
    "                    p_clone = BeautifulSoup(str(p), 'html.parser')\n",
    "                    for img in p_clone.find_all('img'):\n",
    "                        img.decompose()\n",
    "                    conteudo.append(p_clone.decode_contents())  # Corrige escape de tags\n",
    "\n",
    "        # Data\n",
    "        if (data_tag := soup.find('span', class_='data-post')):\n",
    "            data_publicacao = data_tag.get_text(strip=True)\n",
    "\n",
    "        return {\n",
    "            'title': titulo,\n",
    "            'text': ' '.join(conteudo),\n",
    "            'pub_date': data_publicacao\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        tqdm.write(f'‚ö†Ô∏è Erro no artigo {link}: {str(e)}')\n",
    "        return None\n",
    "\n",
    "# %% [code]\n",
    "def main():\n",
    "    \"\"\"Execu√ß√£o principal com serializa√ß√£o JSON corrigida\"\"\"\n",
    "    print('üöÄ Iniciando processo de scraping...')\n",
    "\n",
    "    # Fase 1: Coleta de links\n",
    "    links = get_news_links()\n",
    "    if not links:\n",
    "        print('Nenhum link encontrado.')\n",
    "        return\n",
    "\n",
    "    print(f' {len(links)} links coletados com sucesso')\n",
    "\n",
    "    # Fase 2: Raspagem paralela\n",
    "    dados = []\n",
    "    backup_count = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "        futures = {executor.submit(scrape_article, link): link for link in links}\n",
    "\n",
    "        try:\n",
    "            with tqdm(total=len(links), desc='üì¶ Processando artigos') as pbar:\n",
    "                for future in futures:\n",
    "                    future.add_done_callback(lambda _: pbar.update(1))\n",
    "                    time.sleep(CONFIG['request_delay'] / CONFIG['max_workers'])\n",
    "\n",
    "                for future in futures:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        dados.append(result)\n",
    "                        backup_count += 1\n",
    "\n",
    "                        # Backup peri√≥dico\n",
    "                        if backup_count % CONFIG['backup_interval'] == 0:\n",
    "                            timestamp = pd.Timestamp.now().strftime(\"%H%M%S\")\n",
    "                            backup_file = f\"backup_{timestamp}.json\"\n",
    "                            with open(backup_file, 'w', encoding='utf-8') as f:\n",
    "                                json.dump(dados, f, **CONFIG['json_params'])\n",
    "                            tqdm.write(f' Backup salvo: {backup_file}')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n‚ö†Ô∏è Interrup√ß√£o do usu√°rio! Salvando dados parciais...')\n",
    "\n",
    "    # Fase 3: Salvamento final\n",
    "    if dados:\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"noticias_emserh_{timestamp}.json\"\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(\n",
    "                dados,\n",
    "                f,\n",
    "                **CONFIG['json_params']\n",
    "            )\n",
    "\n",
    "        files.download(filename)\n",
    "        print(f'\\n Dados salvos em {filename}')\n",
    "    else:\n",
    "        print(' Nenhum dado foi coletado')\n",
    "\n",
    "# %% [code]\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
